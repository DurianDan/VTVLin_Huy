{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ScraperHelper.Scrape import *\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = driverHelper(logging_path=\"scrape.log\")\n",
    "helper.addOptions(\n",
    "    arguments=[\"--incognito\"],\n",
    "    page_load_strategy=\"eager\"\n",
    ")\n",
    "helper.forceCreateDriver( vpn_provider=\"nordvpn\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/vietnamese-stopwords.txt\", \"r\") as f:\n",
    "    stopwords = sorted(list(set(f.read().split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_idx = []\n",
    "for idx, word in enumerate(stopwords[:-1]):\n",
    "    if word in stopwords[idx+1]:\n",
    "        repeat_idx.append(idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrape search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.forceCreateDriver( vpn_provider=\"nordvpn\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \"https://vtv.vn/tim-kiem.htm?keywords={}&page={}\"\n",
    "words_in_title = []\n",
    "for idx,word in enumerate(stopwords):\n",
    "    if idx in repeat_idx:\n",
    "        continue\n",
    "    words_in_title.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "scraped_words = set()\n",
    "scraped_link = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_in_title:\n",
    "    if word in scraped_words: continue\n",
    "\n",
    "    logging.info(f'Scraping search word {word}')\n",
    "    page=1\n",
    "    recent_article_link = \"\"\n",
    "    while True:\n",
    "        logging.info(f'Scraping page {page}')\n",
    "        helper.forceGet(\n",
    "            url=search_url.format( word, page),\n",
    "            error_message_in_page=[\"403 Forbidden\"]\n",
    "            )\n",
    "        \n",
    "        logging.info(\"Getting articles info\")\n",
    "        scraped_articles = [\n",
    "            {\n",
    "                \"link\":helper.forceFindElement(By.TAG_NAME, \"a\",element_as_finder=ele).get_attribute(\"href\"), # link\n",
    "                \"title\":helper.forceFindElement(By.TAG_NAME, \"a\",element_as_finder=ele).get_attribute(\"title\").strip(), # title\n",
    "                \"datetime\":helper.forceFindElement(By.CLASS_NAME, \"time\",element_as_finder=ele).get_attribute(\"innerHTML\").strip(), # datetime\n",
    "                \"description\":helper.forceFindElement(By.CLASS_NAME, \"sapo\",element_as_finder=ele).get_attribute(\"innerHTML\").strip(), #description\n",
    "            }\n",
    "            for ele in \n",
    "            helper.driver.find_elements(By.XPATH,\"\"\"//*[@id=\"SearchSolr1\"]/li[*]\"\"\")\n",
    "            # find all articles in a search page\n",
    "            # each element found represents an article\n",
    "        ]\n",
    "\n",
    "        if len(scraped_articles) == 0:\n",
    "            logging.warning(f\"There aren't any article when searching for keyword {word}\")\n",
    "            break\n",
    "\n",
    "        if recent_article_link == scraped_articles[0][\"link\"]:\n",
    "            logging.info(f\"Done scraping article from search word {word}\")\n",
    "            break\n",
    "        else:\n",
    "            recent_article_link = scraped_articles[0][\"link\"]\n",
    "\n",
    "        for article in scraped_articles:\n",
    "            if article[\"link\"] not in scraped_link:\n",
    "                all_articles.append(article)\n",
    "\n",
    "        if page % 3 == 0:  \n",
    "            pd.DataFrame(all_articles).to_feather(\"data/vtv_articles_2.feather\")\n",
    "        page+=1\n",
    "    scraped_words.add(word)\n",
    "    pd.DataFrame(all_articles).to_feather(\"data/vtv_articles_2.feather\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scrape each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_link = list(\n",
    "    pd.read_feather(\"data/vtv_articles.feather\")\n",
    "        [\"link\"].to_dict().items()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_contents = []\n",
    "scraped_index = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = {\n",
    "    \"contents\" : [\n",
    "        \"\"\"//*[@id=\"entry-body\"]/p[*]\"\"\",\n",
    "        \"\"\"//*[@id=\"Main\"]/div/div/div/div[2]/div[2]/div[1]/div[1]/div[3]/p[*]\"\"\",\n",
    "        \"\"\"//*[@id=\"form1\"]/div[2]/div[3]/div/div/div[2]/div/p[*]\"\"\",\n",
    "        \"\"\"//*[@id=\"divNewsContent\"]/div/div/p[*]\"\"\"\n",
    "    ],\n",
    "    \"tags\" : [\n",
    "        \"news_keyword\",\n",
    "        \"new-tags\",\n",
    "        \"tags\",\n",
    "        \"tag\",\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    i.text for i in \n",
    "    helper.driver.find_elements(By.XPATH, \"\"\"//*[@id=\"entry-body\"]/p[*]\"\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_value():\n",
    "    def __init__(self,driver) -> None:\n",
    "        self.driver = driver\n",
    "\n",
    "    def get_author_from_news_info(self):\n",
    "        return (\n",
    "            self.driver.find_element(By.CLASS_NAME, \"news-info\")\n",
    "            .get_attribute(\"innerHTML\")\n",
    "            .replace(\"<b>\",\"\")\n",
    "            .split(\"</b>\")[0]\n",
    "            .strip()\n",
    "        )\n",
    "    def get_author_from_author(self):\n",
    "        return (\n",
    "            self.driver.find_element(By.CLASS_NAME, \"author\")\n",
    "            .get_attribute(\"innerHTML\")\n",
    "            .split(\"<span\")[0]\n",
    "            .strip()\n",
    "        )\n",
    "    def get_author_from_xpath(self):\n",
    "        return (helper.driver\n",
    "                .find_element(By.XPATH, \"\"\"//*[@id=\"form1\"]/div[2]/div[3]/div/div/div[2]/div/p[27]/b\"\"\")\n",
    "                .text\n",
    "                .replace(\"Bài viết: \",\"\")\n",
    "            )\n",
    "    def get_author_from_vtv8(self):\n",
    "        return (helper.driver\n",
    "                .find_element(By.XPATH,\"\"\"//*[@id=\"admWrapsite\"]/div/div[3]/div[3]/div[4]/div/div[1]/p/b\"\"\")\n",
    "                .text\n",
    "                .strip())\n",
    "    \n",
    "    def get_tags(self):\n",
    "        for path in element[\"tags\"]:\n",
    "            try:\n",
    "                return [\n",
    "                    tag.text for tag in \n",
    "                    self.driver.find_element(By.CLASS_NAME, path)\n",
    "                                .find_elements(By.TAG_NAME,\"a\")\n",
    "                ] \n",
    "            except helper.element_exception:\n",
    "                continue\n",
    "        raise NoSuchElementException(\"Cant find tags\")\n",
    "    \n",
    "    def get_contents(self):\n",
    "        contents = []\n",
    "        last_err = None\n",
    "        for path in element[\"contents\"]:\n",
    "            try:\n",
    "                self.driver.find_element(By.XPATH, path)\n",
    "                contents.extend([\n",
    "                    content_element.text\n",
    "                    for content_element in\n",
    "                    self.driver.find_elements(By.XPATH, path)\n",
    "                ])\n",
    "            except helper.element_exception as err:\n",
    "                last_err = err\n",
    "                continue\n",
    "        if len(contents) == 0 :\n",
    "            raise NoSuchElementException(f\"Cant find contents {last_err}\")\n",
    "        return contents\n",
    "    def format(self,values:list):\n",
    "        return \"-|||-\".join(\n",
    "            map(str,values)\n",
    "        )\n",
    "    \n",
    "    def auto_get_value(self, value_type):\n",
    "        last_err = None\n",
    "        if value_type == \"author\":\n",
    "            for _ in range(5):\n",
    "                for func in [\n",
    "                    self.get_author_from_author,\n",
    "                    self.get_author_from_news_info,\n",
    "                    self.get_author_from_xpath,\n",
    "                    self.get_author_from_vtv8 ]:\n",
    "                    try:\n",
    "                        return func()\n",
    "                    except helper.element_exception as err:\n",
    "                        last_err = err\n",
    "                        continue\n",
    "                time.sleep(1)\n",
    "        elif value_type == \"tags\":\n",
    "            for _ in range(5):\n",
    "                try:\n",
    "                    return self.format(self.get_tags())\n",
    "                except NoSuchElementException as err:\n",
    "                    last_err = err\n",
    "                time.sleep(1)\n",
    "        elif value_type == \"contents\":\n",
    "            for _ in range(5):\n",
    "                try:\n",
    "                    return self.format(self.get_contents())\n",
    "                except NoSuchElementException as err:\n",
    "                    last_err = err\n",
    "                time.sleep(1)\n",
    "\n",
    "        return f\"<Error>Cant Find Element: {last_err}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectVPN():\n",
    "    logging.info(\n",
    "        \"\\n\".join([\n",
    "            line for line in \n",
    "            subprocess.run(\"nordvpn c\".split(), capture_output=True)\n",
    "            .stdout.decode(\"utf8\").split(\"\\n\")\n",
    "            if len(line.strip()) > 4\n",
    "        ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.normalCreateDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article_idx, article_link in index_link:\n",
    "\n",
    "    if article_idx in scraped_index:\n",
    "        continue\n",
    "    \n",
    "    custom_error_message = None\n",
    "    if article_idx % 100 == 0 and article_idx != 0:\n",
    "        connectVPN()\n",
    "        helper.reopenDriver(reconnect_vpn=True)\n",
    "        pl.DataFrame(all_articles_contents\n",
    "                     ).write_ipc(\"data/articles_content.arrow\")\n",
    "        \n",
    "    logging.info(f\"ARTICLE_{article_idx}\")\n",
    "    \n",
    "\n",
    "    helper.forceGet(\n",
    "        article_link,\n",
    "        try_refresh_before_retry=True,\n",
    "        error_message_in_page=custom_error_message\n",
    "        )\n",
    "\n",
    "    get_value_handler = get_value(helper.driver)\n",
    "\n",
    "    tmp_info = {\n",
    "        \"contents\":\"<Error>\",\n",
    "        \"tags\":\"<Error>\",\n",
    "        \"author\":\"<Error>\"\n",
    "    }\n",
    "    for k in tmp_info:\n",
    "        if k == \"contents\" and article_link.startswith(\"https://vtv.vn/video-dac-sac\"):\n",
    "            tmp_info[k] = helper.driver.find_element(By.XPATH, \"./html/body\").text\n",
    "        else:       \n",
    "            tmp_info[k] = get_value_handler.auto_get_value(\n",
    "                value_type=k\n",
    "            )\n",
    "\n",
    "        if tmp_info[k].startswith(\"<Error>\"):\n",
    "            logging.error(f\"Cant scrape {k}\")\n",
    "\n",
    "    tmp_info[\"link\"] = article_link\n",
    "    whole_website = '<Contents Not Blank>'\n",
    "\n",
    "    if tmp_info[\"contents\"].startswith(\"<Error>\"):\n",
    "        logging.warning(\"Cant scrape contents\")\n",
    "        try:\n",
    "            whole_website = helper.driver.find_element(\n",
    "                By.XPATH,\"./html/body\").text\n",
    "            logging.info(whole_website)\n",
    "        except Exception as e:\n",
    "            whole_website = \"<Cant get page body either>\"\n",
    "            logging.error(\"Cant get page body either:\")\n",
    "            logging.error(e)\n",
    "    \n",
    "    tmp_info[\"whole_website\"] = whole_website\n",
    "    all_articles_contents.append(tmp_info)\n",
    "    scraped_index.add(article_idx)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = pl.DataFrame(all_articles_contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
