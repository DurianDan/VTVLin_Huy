{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ScraperHelper.Scrape import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = driverHelper(logging_path=\"scrape.log\")\n",
    "helper.addOptions(\n",
    "    arguments=[\"--incognito\"],\n",
    "    page_load_strategy=\"eager\"\n",
    ")\n",
    "helper.forceCreateDriver( vpn_provider=\"nordvpn\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/vietnamese-stopwords.txt\", \"r\") as f:\n",
    "    stopwords = sorted(list(set(f.read().split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_idx = []\n",
    "for idx, word in enumerate(stopwords[:-1]):\n",
    "    if word in stopwords[idx+1]:\n",
    "        repeat_idx.append(idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrape search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.forceCreateDriver( vpn_provider=\"nordvpn\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \"https://vtv.vn/tim-kiem.htm?keywords={}&page={}\"\n",
    "words_in_title = []\n",
    "for idx,word in enumerate(stopwords):\n",
    "    if idx in repeat_idx:\n",
    "        continue\n",
    "    words_in_title.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "scraped_words = set()\n",
    "scraped_link = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_in_title:\n",
    "    if word in scraped_words: continue\n",
    "\n",
    "    logging.info(f'Scraping search word {word}')\n",
    "    page=1\n",
    "    recent_article_link = \"\"\n",
    "    while True:\n",
    "        logging.info(f'Scraping page {page}')\n",
    "        helper.forceGet(\n",
    "            url=search_url.format( word, page),\n",
    "            error_message_in_page=[\"403 Forbidden\"]\n",
    "            )\n",
    "        \n",
    "        logging.info(\"Getting articles info\")\n",
    "        scraped_articles = [\n",
    "            {\n",
    "                \"link\":helper.forceFindElement(By.TAG_NAME, \"a\",element_as_finder=ele).get_attribute(\"href\"), # link\n",
    "                \"title\":helper.forceFindElement(By.TAG_NAME, \"a\",element_as_finder=ele).get_attribute(\"title\").strip(), # title\n",
    "                \"datetime\":helper.forceFindElement(By.CLASS_NAME, \"time\",element_as_finder=ele).get_attribute(\"innerHTML\").strip(), # datetime\n",
    "                \"description\":helper.forceFindElement(By.CLASS_NAME, \"sapo\",element_as_finder=ele).get_attribute(\"innerHTML\").strip(), #description\n",
    "            }\n",
    "            for ele in \n",
    "            helper.driver.find_elements(By.XPATH,\"\"\"//*[@id=\"SearchSolr1\"]/li[*]\"\"\")\n",
    "            # find all articles in a search page\n",
    "            # each element found represents an article\n",
    "        ]\n",
    "\n",
    "        if len(scraped_articles) == 0:\n",
    "            logging.warning(f\"There aren't any article when searching for keyword {word}\")\n",
    "            break\n",
    "\n",
    "        if recent_article_link == scraped_articles[0][\"link\"]:\n",
    "            logging.info(f\"Done scraping article from search word {word}\")\n",
    "            break\n",
    "        else:\n",
    "            recent_article_link = scraped_articles[0][\"link\"]\n",
    "\n",
    "        for article in scraped_articles:\n",
    "            if article[\"link\"] not in scraped_link:\n",
    "                all_articles.append(article)\n",
    "\n",
    "        if page % 3 == 0:  \n",
    "            pd.DataFrame(all_articles).to_feather(\"data/vtv_articles_2.feather\")\n",
    "        page+=1\n",
    "    scraped_words.add(word)\n",
    "    pd.DataFrame(all_articles).to_feather(\"data/vtv_articles_2.feather\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scrape each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_link = list(\n",
    "    pd.read_feather(\"data/vtv_articles_2.feather\")\n",
    "        [\"link\"].to_dict().items()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_contents = []\n",
    "scraped_index = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_options = {\n",
    "    \"basic\":{\n",
    "        \"contents\":\"\"\"//*[@id=\"entry-body\"]\"\"\",\n",
    "            # .find_elements(By.TAG_NAME, \"p\")\n",
    "        \"tags\":\"\"\"//*[@id=\"admWrapsite\"]/div[3]/div[2]/div[2]/div[3]/div[1]/div[1]/div[9]\"\"\",\n",
    "            # .find_elements(By.TAG_NAME,'''a''')\n",
    "        \"author\":\"\"\"//*[@id=\"admWrapsite\"]/div[3]/div[2]/div[2]/div[3]/div[1]/div[1]/p\"\"\"\n",
    "            # .get_attribute(\"innerHTML\").split(\"<span\")[0]\n",
    "    },\n",
    "    \"sports\":{\n",
    "        \"contents\":\"\"\"//*[@id=\"entry-body\"]\"\"\",\n",
    "            # .find_elements(By.TAG_NAME, \"p\")\n",
    "        \"tags\":\"\"\"//*[@id=\"admWrapsite\"]/div[3]/div[2]/div/div[3]/div[1]/div[1]/div[8]\"\"\",\n",
    "            # .find_elements(By.TAG_NAME,'''a''')\n",
    "        \"author\":\"\"\"//*[@id=\"admWrapsite\"]/div[3]/div[2]/div/div[3]/div[1]/div[1]/p/b\"\"\"\n",
    "    },\n",
    "    \"health\":{\n",
    "        \"contents\":\"\"\"//*[@id=\"Main\"]/div/div/div/div[2]/div[2]/div[1]/div[1]/div[3]\"\"\",\n",
    "            # .find_elements(By.TAG_NAME, \"p\"):\n",
    "        \"tags\":\"\"\"//*[@id=\"Main\"]/div/div/div/div[2]/div[2]/div[1]/div[1]/div[5]\"\"\",\n",
    "            # .find_elements(By.TAG_NAME, \"a\")\n",
    "        \"author\":\"\"\"//*[@id=\"Main\"]/div/div/div/div[2]/div[2]/div[1]/div[1]/div[1]/div[1]/span\"\"\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article_idx, article_link in index_link:\n",
    "    logging.info(f\"ARTICLE_{article_idx}: {article_link}\")\n",
    "    helper.forceGet(\n",
    "        article_link,\n",
    "        try_refresh_before_retry=True\n",
    "        )\n",
    "    tmp_info = {\n",
    "        \"contents\":\"<Error>\",\n",
    "        \"tags\":\"<Error>\",\n",
    "        \"author\":\"<Error>\"\n",
    "    }\n",
    "    for i in range(3):\n",
    "        found_page_type=False\n",
    "        for page_type, elements in element_options.items():\n",
    "            try:\n",
    "                tmp_info[\"tags\"] = [\n",
    "                    tag.text for tag in \n",
    "                    helper.driver.find_element(By.XPATH, elements[\"tags\"])\n",
    "                                .find_elements(By.TAG_NAME,\"a\")\n",
    "                    ]\n",
    "                found_page_type = True\n",
    "            except helper.element_exception as err:\n",
    "                logging.warning(f\"This page is not of type {page_type}\")\n",
    "                continue\n",
    "            if page_type == \"basic\":\n",
    "                print(page_type)\n",
    "                tmp_info[\"author\"] = (helper\n",
    "                    .forceFindElement(By.XPATH, elements[\"author\"])\n",
    "                    .get_attribute(\"innerHTML\")\n",
    "                    .split(\"<span\")[0]\n",
    "                )\n",
    "            else:\n",
    "                print(page_type)\n",
    "                tmp_info[\"author\"] = (helper\n",
    "                    .forceFindElement(By.XPATH,elements[\"author\"])\n",
    "                    .text\n",
    "                )\n",
    "            tmp_info[\"contents\"] = [\n",
    "                content_element.text for content_element in\n",
    "                helper.driver.find_elements(By.XPATH, elements[\"contents\"])\n",
    "            ]\n",
    "            break\n",
    "        if found_page_type:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
